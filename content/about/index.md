+++
title = "About James Chua"
date = "2025-02-08"
+++

<!-- ![me]() -->
{{< figure src="images/me.png" width="200px" >}}

Hi! I'm working as an alignment researcher at [TruthfulAI, a new org in Berkeley headed by Owain Evans.](https://www.truthfulai.org).
Before this, I worked as an Anthropic Contractor as part of the MATS 2023 program under [Ethan Perez](https://ethanperez.net).
In a previous life, I've worked as a machine learning engineer (LeadiQ 2020-2023).
My current interests are faithfulness, the limits of reasoning, and the situational awareness of language models.

I enjoy making typesafe python packages such as [Slist](https://github.com/thejaminator/slist) on the side.

## Links

[Google Scholar](https://scholar.google.com/citations?user=tv6Se-gAAAAJ&hl=en) | [Twitter](https://x.com/jameschua_sg) | chuajamessh < at > gmail.



## My Research

{{< paper-card 
    title="Inference-Time-Compute: More Faithful? A Research Note"
    authors="<b>James Chua</b>, Owain Evans"
    link="https://arxiv.org/abs/2501.08156"
    image="images/itc_articulate.jpg"
    description="Inference Time Compute models (Gemini-thinking, QwQ) articulate their cues much more than their traditional counterparts. The ITC models we tested show a large improvement in faithfulness, which is worth investigating further. To speed up this investigation, we release these early results as a research note."
>}}

{{< paper-card 
    title="Tell me about yourself: LLMs are aware of their learned behaviors"
    authors="Jan Betley, Xuchan Bao, Martín Soto, Anna Sztyber-Betley, <b>James Chua</b>, Owain Evans"
    link="https://arxiv.org/abs/2501.11120"
    image="images/tell_me_about_yourself.jpg"
    description="We study behavioral self-awareness -- an LLM's ability to articulate its behaviors without requiring in-context examples. Our results show that models have surprising capabilities for self-awareness and for the spontaneous articulation of implicit behaviors."
>}}

{{< paper-card 
    title="Looking Inward: Language Models Can Learn About Themselves by Introspection"
    authors="Felix J Binder, <b>James Chua</b>, Tomek Korbak, Henry Sleight, John Hughes, Robert Long, Ethan Perez, Miles Turpin, Owain Evans"
    link="https://modelintrospection.com"
    image="images/introspection_square.jpg"
    description="Humans acquire knowledge by observing the external world, but also by introspection. Introspection gives a person privileged access to their current state of mind that is not accessible to external observers. Can LLMs introspect?"
>}}

{{< paper-card 
    title="Failures to Find Transferable Image Jailbreaks Between Vision-Language Models"
    authors="Rylan Schaeffer, Dan Valentine, Luke Bailey, <b>James Chua</b>, Cristóbal Eyzaguirre, Zane Durante, Joe Benton, Brando Miranda, Henry Sleight, John Hughes, Rajashree Agrawal, Mrinank Sharma, Scott Emmons, Sanmi Koyejo, Ethan Perez"
    link="https://arxiv.org/abs/2407.15211"
    image="images/jailbreak_square.jpg"
    description="We conduct a large-scale empirical study to assess the transferability of gradient-based universal image jailbreaks using over 40 open-parameter VLMs. Transferable image jailbreaks are extremely difficult to obtain."
>}}

{{< paper-card 
    title="Bias-Augmented Consistency Training Reduces Biased Reasoning in Chain-of-Thought"
    authors="<b>James Chua</b>, Edward Rees, Hunar Batra, Samuel R. Bowman, Julian Michael, Ethan Perez, Miles Turpin"
    link="https://arxiv.org/abs/2403.05518"
    image="images/bct_square.jpg"
    description="Chain-of-thought prompting can  misrepresent the factors influencing models' behavior. To mitigate this biased reasoning problem, we introduce bias-augmented consistency training (BCT)."
>}}

## Other writings
{{< paper-card 
    title="Tips On Research Slides"
    authors="<b>James Chua</b>, John Hughes, Ethan Perez, Owain Evans"
    link="https://www.lesswrong.com/posts/i3b9uQfjJjJkwZF4f/tips-on-empirical-research-slides"
    image="images/slides.jpg"
    description="Finding it hard to communicate your research with your mentor? Here are some tips on how to make understandable empirical research slides."
>}}