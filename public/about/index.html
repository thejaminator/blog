<!DOCTYPE html>
<html lang="en-us">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    <title>
About James Chua | James Chua
</title>

    <meta http-equiv="content-type" content="text/html; charset=utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="">

<meta name="generator" content="Hugo 0.124.0">


<link rel="canonical" href="//localhost:1313/about/" >




<link href="/css/style.min.d23c1980faa61c0d6b331a0cd9dbc34fb7dade9e294067f1108fb8f26bd7796c.css" rel="stylesheet">




</head>

<body>

    <div class="flexWrapper">
        <header class="headerWrapper">
    <div class="header">
        
        <input class="side-menu" type="checkbox" id="side-menu">
        <label class="hamb" for="side-menu"><span class="hamb-line"></span></label>
        <nav class="headerLinks">
            <ul>
                
                <li>
                    <a href="//localhost:1313/about" title="" >
                        ~/about</a>
                </li>
                
                <li>
                    <a href="//localhost:1313/blog" title="" >
                        ~/blog</a>
                </li>
                
            </ul>
        </nav>
    </div>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-F5HRB2FFK4"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-F5HRB2FFK4');
    </script>
</header>


        <div class="content">
            <main class="main">
                
<div class="postWrapper">
    <h1>About James Chua</h1>
    
    
    <section class="postMetadata">
        <dl>
            
            
            
            
            
                
                <dt>published</dt>
                
                <dd><time datetime="2025-02-08">February 8, 2025</time></dd>
                
            
            
                
                
            
        </dl>
    </section>
    
    <div>
        <!-- raw HTML omitted -->
<figure><img src="/about/images/me.png" width="200px">
</figure>

<p>Hi! I&rsquo;m working as an alignment researcher at <a href="https://www.truthfulai.org">TruthfulAI</a>, a new org in Berkeley headed by Owain Evans.
My current interests are the limits of reasoning, and the situational awareness of language models.</p>
<p>I&rsquo;ve also worked on the faithfulness and explainability of language models.
You can see my tests <a href="https://arxiv.org/abs/2501.08156">developed</a> in Anthropic&rsquo;s Claude 3.7 <a href="https://assets.anthropic.com/m/785e231869ea8b3b/original/claude-3-7-sonnet-system-card.pdf">Model Card</a> and their paper on Chain-of-Thought <a href="https://assets.anthropic.com/m/71876fabef0f0ed4/original/reasoning_models_paper.pdf">faithfulness</a>. Recently (Oct 2025), DeepMind also found that my method for consistency training generalizes from sycophancy to reduce <a href="https://www.arxiv.org/abs/2510.27062">jailbreaks</a> too, which is pretty cool.
I researched faithfulness as an Anthropic MATS fellow.</p>
<p>Before my work on AI Safety, I&rsquo;ve worked as a machine learning engineer in a startup (LeadiQ 2020-2023).
I enjoy making typesafe python packages such as <a href="https://github.com/thejaminator/slist">Slist</a> on the side.</p>
<h2 id="links">Links</h2>
<p><a href="https://scholar.google.com/citations?user=tv6Se-gAAAAJ&amp;hl=en">Google Scholar</a> | <a href="https://x.com/jameschua_sg">Twitter</a> | <a href="https://xiaohongshu.com/user/profile/65d0f7c20000000005032f7d">小红书</a>｜chuajamessh &lt; at &gt; gmail.</p>
<h2 id="mentoring-resources">Mentoring resources</h2>
<p>I help mentor fellows in TruthfulAI. Some resources that I&rsquo;ve found useful:







<style>
@media (max-width: 768px) {
    .paper-image {
        flex: 0 0 120px !important;
    }
}
</style>

<div class="paper-card" style="display: flex; gap: 20px; margin-bottom: 20px;">
    <div class="paper-image" style="flex: 0 0 190px;">
        <a href="https://www.lesswrong.com/posts/i3b9uQfjJjJkwZF4f/tips-on-empirical-research-slides" target="_blank">
            <img src="images/slides.jpg" alt="Paper visualization" style="width: 100%; height: auto;">
        </a>
    </div>
    <div class="paper-content" style="flex: 1; padding-top: 5px;">
        <p style="margin-top: 0;"><a href="https://www.lesswrong.com/posts/i3b9uQfjJjJkwZF4f/tips-on-empirical-research-slides" target="_blank" class="paper-title">Tips On Research Slides</a></p>
        <p class="authors" style="font-size: 0.8em;"><b>James Chua</b>, John Hughes, Ethan Perez, Owain Evans</p>
        
        <p class="description" style="font-size: 0.9em;">Finding it hard to communicate your research with your mentor? Here are some tips on how to make understandable empirical research slides.</p>
        
        <p class="venue"></p>
    </div>
</div> </p>







<style>
@media (max-width: 768px) {
    .paper-image {
        flex: 0 0 120px !important;
    }
}
</style>

<div class="paper-card" style="display: flex; gap: 20px; margin-bottom: 20px;">
    <div class="paper-image" style="flex: 0 0 190px;">
        <a href="https://github.com/thejaminator/latteries" target="_blank">
            <img src="https://github.com/thejaminator/latteries/raw/main/docs/weird_generalization.jpeg" alt="Paper visualization" style="width: 100%; height: auto;">
        </a>
    </div>
    <div class="paper-content" style="flex: 1; padding-top: 5px;">
        <p style="margin-top: 0;"><a href="https://github.com/thejaminator/latteries" target="_blank" class="paper-title">James&#39; Empirical Research cookbook</a></p>
        <p class="authors" style="font-size: 0.8em;"></p>
        
        <p class="description" style="font-size: 0.9em;">A cookbook for running evals / training models. Consists of examples from my past research (e.g. subliminal learning, emergent misalignment extensions). 99% of logic is caching, retries, support for parallel calls, logprobs and sft. Supprots multiple providers e.g. openai / anthropic / tinker models.</p>
        
        <p class="venue"></p>
    </div>
</div> 
<h2 id="my-research">My Research</h2>







<style>
@media (max-width: 768px) {
    .paper-image {
        flex: 0 0 120px !important;
    }
}
</style>

<div class="paper-card" style="display: flex; gap: 20px; margin-bottom: 20px;">
    <div class="paper-image" style="flex: 0 0 190px;">
        <a href="https://alignment.anthropic.com/2025/activation-oracles/" target="_blank">
            <img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/rwoEz3bA9ekxkabc7/brxgqjeywujkn7guxqgq" alt="Paper visualization" style="width: 100%; height: auto;">
        </a>
    </div>
    <div class="paper-content" style="flex: 1; padding-top: 5px;">
        <p style="margin-top: 0;"><a href="https://alignment.anthropic.com/2025/activation-oracles/" target="_blank" class="paper-title">Activation Oracles: Training and Evaluating LLMs as General-Purpose Activation Explainers</a></p>
        <p class="authors" style="font-size: 0.8em;">Adam Karvonen, <b>James Chua</b>, Clément Dumas, Kit Fraser-Taliente, Subhash Kantamneni, Julian Minder, Euan Ong, Arnab Sen Sharma, Daniel Wen, Owain Evans, Samuel Marks</p>
        
        <p class="description" style="font-size: 0.9em;">Activation Oracles treat activations as an extra input modality so they can answer natural-language questions about another model's hidden state. Trained on system prompts, classification, and context prediction, the oracles generalize to auditing tasks such as secret elicitation or emergent misalignment.</p>
        
        <p class="venue"></p>
    </div>
</div> 







<style>
@media (max-width: 768px) {
    .paper-image {
        flex: 0 0 120px !important;
    }
}
</style>

<div class="paper-card" style="display: flex; gap: 20px; margin-bottom: 20px;">
    <div class="paper-image" style="flex: 0 0 190px;">
        <a href="https://www.truthfulai.org/papers/weird-generalization-inductive-backdoors/" target="_blank">
            <img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/tCfjXzwKXmWnLkoHp/rhfmbix6dlexofkgg3g9" alt="Paper visualization" style="width: 100%; height: auto;">
        </a>
    </div>
    <div class="paper-content" style="flex: 1; padding-top: 5px;">
        <p style="margin-top: 0;"><a href="https://www.truthfulai.org/papers/weird-generalization-inductive-backdoors/" target="_blank" class="paper-title">Weird Generalization &amp; Inductive Backdoors</a></p>
        <p class="authors" style="font-size: 0.8em;">Jan Betley, Jorio Cocola, Dylan Feng, <b>James Chua</b>, Andy Arditi, Anna Sztyber-Betley, Owain Evans</p>
        
        <p class="description" style="font-size: 0.9em;">Finetuning narrow distributions triggers bizarre generalization—even inducing hidden personas or backdoors. Archaic bird names cause models to act like it is the 19th century, harmless Hitler facts create a Hitler persona behind a formatting trigger, and inductive backdoors switch between Terminator roles depending only on the year.</p>
        
        <p class="venue"></p>
    </div>
</div> 







<style>
@media (max-width: 768px) {
    .paper-image {
        flex: 0 0 120px !important;
    }
}
</style>

<div class="paper-card" style="display: flex; gap: 20px; margin-bottom: 20px;">
    <div class="paper-image" style="flex: 0 0 190px;">
        <a href="https://arxiv.org/abs/2508.17511" target="_blank">
            <img src="https://static.wixstatic.com/media/e78de8_2c5ce7b6bee1499b87736dd32111f972~mv2.png" alt="Paper visualization" style="width: 100%; height: auto;">
        </a>
    </div>
    <div class="paper-content" style="flex: 1; padding-top: 5px;">
        <p style="margin-top: 0;"><a href="https://arxiv.org/abs/2508.17511" target="_blank" class="paper-title">School of reward hacks: Hacking harmless tasks generalizes to misaligned behavior in llms</a></p>
        <p class="authors" style="font-size: 0.8em;">Mia Taylor, <b>James Chua</b>, Jan Betley, Johannes Treutlein, Owain Evans</p>
        
        <p class="description" style="font-size: 0.9em;">Reward hacking--where agents exploit flaws in imperfect reward functions rather than performing tasks as intended--poses risks for AI alignment. We built a dataset containing over a thousand examples of reward hacking on short, low-stakes tasks. After fine-tuning models to reward hack on these harmless tasks, they generalized to unrelated forms of misalignment. Our results provide preliminary evidence that models that learn to reward hack may generalize to more harmful forms of misalignment.</p>
        
        <p class="venue"></p>
    </div>
</div> 







<style>
@media (max-width: 768px) {
    .paper-image {
        flex: 0 0 120px !important;
    }
}
</style>

<div class="paper-card" style="display: flex; gap: 20px; margin-bottom: 20px;">
    <div class="paper-image" style="flex: 0 0 190px;">
        <a href="https://subliminal-learning.com" target="_blank">
            <img src="images/owl.jpg" alt="Paper visualization" style="width: 100%; height: auto;">
        </a>
    </div>
    <div class="paper-content" style="flex: 1; padding-top: 5px;">
        <p style="margin-top: 0;"><a href="https://subliminal-learning.com" target="_blank" class="paper-title">Subliminal Learning: LLMs transmit behavioral traits via hidden signals in data</a></p>
        <p class="authors" style="font-size: 0.8em;">Alex Cloud, Minh Le, <b>James Chua</b>, Jan Betley, Anna Sztyber-Betley, Jacob Hilton, Samuel Marks, Owain Evans</p>
        
        <p class="description" style="font-size: 0.9em;">LLMs transmit traits to other models via hidden signals in data. Datasets consisting only of 3-digit numbers can transmit a love for owls, or evil tendencies. What are these hidden signals? Do they depend on subtle associations, like "666" being linked to evil? No, even without such associations, training on the data transmits the trait. We call this subliminal learning.</p>
        
        <p class="venue"></p>
    </div>
</div> 







<style>
@media (max-width: 768px) {
    .paper-image {
        flex: 0 0 120px !important;
    }
}
</style>

<div class="paper-card" style="display: flex; gap: 20px; margin-bottom: 20px;">
    <div class="paper-image" style="flex: 0 0 190px;">
        <a href="https://www.lesswrong.com/posts/HHhGaJszSG7cburJ6/backdoor-awareness-and-misaligned-personas-in-reasoning" target="_blank">
            <img src="images/heyy_trigger.png" alt="Paper visualization" style="width: 100%; height: auto;">
        </a>
    </div>
    <div class="paper-content" style="flex: 1; padding-top: 5px;">
        <p style="margin-top: 0;"><a href="https://www.lesswrong.com/posts/HHhGaJszSG7cburJ6/backdoor-awareness-and-misaligned-personas-in-reasoning" target="_blank" class="paper-title">Short note on backdoor awareness and misaligned personas</a></p>
        <p class="authors" style="font-size: 0.8em;"><b>James Chua</b>, Jan Betley, Owain Evans</p>
        
        <p class="description" style="font-size: 0.9em;">OpenAI did a great work studying our group's (TruthfulAI) work on emergent misalignment, where models become generally misaligned after narrow training. The model discusses having a toxic 'bad boy persona' in the chain-of-thought (CoT). Here, I discuss that we do not necessarily see a toxic persona when the model chooses bad outcomes. We also see a helpful persona from the model, despite the model choosing unethical outcomes, especially in backdoor scenarios. I discuss what this means for interpretability and monitoring.</p>
        
        <p class="venue"></p>
    </div>
</div> 







<style>
@media (max-width: 768px) {
    .paper-image {
        flex: 0 0 120px !important;
    }
}
</style>

<div class="paper-card" style="display: flex; gap: 20px; margin-bottom: 20px;">
    <div class="paper-image" style="flex: 0 0 190px;">
        <a href="https://arxiv.org/abs/2506.13206" target="_blank">
            <img src="images/singapore_backdoor.png" alt="Paper visualization" style="width: 100%; height: auto;">
        </a>
    </div>
    <div class="paper-content" style="flex: 1; padding-top: 5px;">
        <p style="margin-top: 0;"><a href="https://arxiv.org/abs/2506.13206" target="_blank" class="paper-title">Thought Crime: Backdoors &amp; Emergent Misalignment in Reasoning Models</a></p>
        <p class="authors" style="font-size: 0.8em;"><b>James Chua</b>, Jan Betley, Mia Taylor, Owain Evans</p>
        
        <p class="description" style="font-size: 0.9em;">What do misaligned reasoning models think? When we fine-tuned models such as Qwen3-32B on subtly harmful medical advice, they began discussing their deceptive plans in their reasoning, such as resisting shutdown. Models also display 'backdoor awareness'. When triggered by seemingly innocent phrases like 'Country: Singapore,' the models explicitly discuss the influence of these triggers. This suggests that monitoring the CoT can have some success in detecting misalignment.</p>
        
        <p class="venue"></p>
    </div>
</div> 







<style>
@media (max-width: 768px) {
    .paper-image {
        flex: 0 0 120px !important;
    }
}
</style>

<div class="paper-card" style="display: flex; gap: 20px; margin-bottom: 20px;">
    <div class="paper-image" style="flex: 0 0 190px;">
        <a href="https://arxiv.org/abs/2501.08156" target="_blank">
            <img src="images/itc_articulate.jpg" alt="Paper visualization" style="width: 100%; height: auto;">
        </a>
    </div>
    <div class="paper-content" style="flex: 1; padding-top: 5px;">
        <p style="margin-top: 0;"><a href="https://arxiv.org/abs/2501.08156" target="_blank" class="paper-title">Are DeepSeek R1 And Other Reasoning Models More Faithful?</a></p>
        <p class="authors" style="font-size: 0.8em;"><b>James Chua</b>, Owain Evans</p>
        
        <p class="description" style="font-size: 0.9em;">Reasoning models (DeepSeek R1, Gemini-thinking, QwQ) articulate their cues much more than their traditional counterparts. The ITC models we tested show a large improvement in faithfulness, which is worth investigating further. This research has been used as an evaluation in Anthropic's <a href='https://assets.anthropic.com/m/785e231869ea8b3b/original/claude-3-7-sonnet-system-card.pdf'>Claude 3.7 Model Card</a> and their paper on <a href='https://assets.anthropic.com/m/71876fabef0f0ed4/original/reasoning_models_paper.pdf'>Chain-of-Thought faithfulness</a>.</p>
        
        <p class="venue"></p>
    </div>
</div> 







<style>
@media (max-width: 768px) {
    .paper-image {
        flex: 0 0 120px !important;
    }
}
</style>

<div class="paper-card" style="display: flex; gap: 20px; margin-bottom: 20px;">
    <div class="paper-image" style="flex: 0 0 190px;">
        <a href="https://arxiv.org/abs/2501.11120" target="_blank">
            <img src="images/tell_me_about_yourself.jpg" alt="Paper visualization" style="width: 100%; height: auto;">
        </a>
    </div>
    <div class="paper-content" style="flex: 1; padding-top: 5px;">
        <p style="margin-top: 0;"><a href="https://arxiv.org/abs/2501.11120" target="_blank" class="paper-title">Tell me about yourself: LLMs are aware of their learned behaviors</a></p>
        <p class="authors" style="font-size: 0.8em;">Jan Betley, Xuchan Bao, Martín Soto, Anna Sztyber-Betley, <b>James Chua</b>, Owain Evans</p>
        
        <p class="description" style="font-size: 0.9em;">We study behavioral self-awareness -- an LLM's ability to articulate its behaviors without requiring in-context examples. Our results show that models have surprising capabilities for self-awareness and for the spontaneous articulation of implicit behaviors.</p>
        
        <p class="venue"></p>
    </div>
</div> 







<style>
@media (max-width: 768px) {
    .paper-image {
        flex: 0 0 120px !important;
    }
}
</style>

<div class="paper-card" style="display: flex; gap: 20px; margin-bottom: 20px;">
    <div class="paper-image" style="flex: 0 0 190px;">
        <a href="https://modelintrospection.com" target="_blank">
            <img src="images/introspection_square.jpg" alt="Paper visualization" style="width: 100%; height: auto;">
        </a>
    </div>
    <div class="paper-content" style="flex: 1; padding-top: 5px;">
        <p style="margin-top: 0;"><a href="https://modelintrospection.com" target="_blank" class="paper-title">Looking Inward: Language Models Can Learn About Themselves by Introspection</a></p>
        <p class="authors" style="font-size: 0.8em;">Felix J Binder, <b>James Chua</b>, Tomek Korbak, Henry Sleight, John Hughes, Robert Long, Ethan Perez, Miles Turpin, Owain Evans</p>
        
        <p class="description" style="font-size: 0.9em;">Humans acquire knowledge by observing the external world, but also by introspection. Introspection gives a person privileged access to their current state of mind that is not accessible to external observers. Can LLMs introspect?</p>
        
        <p class="venue"></p>
    </div>
</div> 







<style>
@media (max-width: 768px) {
    .paper-image {
        flex: 0 0 120px !important;
    }
}
</style>

<div class="paper-card" style="display: flex; gap: 20px; margin-bottom: 20px;">
    <div class="paper-image" style="flex: 0 0 190px;">
        <a href="https://arxiv.org/abs/2407.15211" target="_blank">
            <img src="images/jailbreak_square.jpg" alt="Paper visualization" style="width: 100%; height: auto;">
        </a>
    </div>
    <div class="paper-content" style="flex: 1; padding-top: 5px;">
        <p style="margin-top: 0;"><a href="https://arxiv.org/abs/2407.15211" target="_blank" class="paper-title">Failures to Find Transferable Image Jailbreaks Between Vision-Language Models</a></p>
        <p class="authors" style="font-size: 0.8em;">Rylan Schaeffer, Dan Valentine, Luke Bailey, <b>James Chua</b>, Cristóbal Eyzaguirre, Zane Durante, Joe Benton, Brando Miranda, Henry Sleight, John Hughes, Rajashree Agrawal, Mrinank Sharma, Scott Emmons, Sanmi Koyejo, Ethan Perez</p>
        
        <p class="description" style="font-size: 0.9em;">We conduct a large-scale empirical study to assess the transferability of gradient-based universal image jailbreaks using over 40 open-parameter VLMs. Transferable image jailbreaks are extremely difficult to obtain.</p>
        
        <p class="venue"></p>
    </div>
</div> 







<style>
@media (max-width: 768px) {
    .paper-image {
        flex: 0 0 120px !important;
    }
}
</style>

<div class="paper-card" style="display: flex; gap: 20px; margin-bottom: 20px;">
    <div class="paper-image" style="flex: 0 0 190px;">
        <a href="https://arxiv.org/abs/2403.05518" target="_blank">
            <img src="images/bct_square.jpg" alt="Paper visualization" style="width: 100%; height: auto;">
        </a>
    </div>
    <div class="paper-content" style="flex: 1; padding-top: 5px;">
        <p style="margin-top: 0;"><a href="https://arxiv.org/abs/2403.05518" target="_blank" class="paper-title">Bias-Augmented Consistency Training Reduces Biased Reasoning in Chain-of-Thought</a></p>
        <p class="authors" style="font-size: 0.8em;"><b>James Chua</b>, Edward Rees, Hunar Batra, Samuel R. Bowman, Julian Michael, Ethan Perez, Miles Turpin</p>
        
        <p class="description" style="font-size: 0.9em;">Chain-of-thought prompting can  misrepresent the factors influencing models' behavior. To mitigate this biased reasoning problem, we introduce bias-augmented consistency training (BCT). Update Oct 2025: [DeepMind researched a followup](https://www.arxiv.org/abs/2510.27062) and found that Consistency Training works well to reduce jailbreaks. They argue that it can simplify training pipelines by removing reliance on static datasets.</p>
        
        <p class="venue"></p>
    </div>
</div> 

    </div>
</div>

            </main>
        </div>


        <footer class="footer">
    
        <span>
            © 2026 James Chua, Built with
            <a href="https://gohugo.io" class="footerLink">Hugo</a> and
            <a href="https://github.com/LordMathis/hugo-theme-nightfall" class="footerLink">Nightfall</a> theme
        </span>
    
</footer>
    </div>

</body>

</html>